---
title: Prompt Engineering Guidelines
lang: en
subtitle: Prompt Engineering 1
author: Jan Kirenz
execute:
  eval: false
  echo: true
highlight-style: github
format:
  revealjs: 
    toc: true
    toc-depth: 1
    embed-resources: false
    theme: [dark, ../custom.scss]  
    incremental: true
    transition: slide
    background-transition: fade
    transition-speed: slow
    code-copy: true
    code-line-numbers: true
    smaller: false
    scrollable: true
    slide-number: c
    preview-links: auto
    chalkboard: 
      buttons: false
   #logo: images/logo.png
    footer: Jan Kirenz
---

# Introduction

## What is prompt engineering?

- The inputs to LLMs are referred to as "**prompts**"

- Designing a prompt is essentially how you ‚Äúprogram‚Äù a LLM:
  - with **instructions** 
  - or some **examples** of how to successfully complete a task

- Prompt **engineering**
  - methods to improve model reasoning 
  - reduce the likelihood of model hallucinations


## Prompting in ChatGPT

![](/images/prompt.png)

## We use OpenAI's API {.smaller}

![](/images/api.png)


- The OpenAI API can be applied to virtually any task that requires understanding or generating natural language and code. 

- Can also be used to generate and edit images or convert speech into text.

## Example prompt {.smaller}

- Example prompt with a *system message* (helps set the behavior of the assistant)

. . .

```{python}
# | eval: false
prompt = f"""
Summarize the text delimited by triple backticks \ 
into a single sentence and add a joke or playful comment

```{text}```
"""
```

- `f`: [Formatted strings](https://docs.python.org/3/tutorial/inputoutput.html) allow you to embed expressions inside string literals, using curly braces {}
- `"""`: Triple double-quotes are used to denote a string that spans multiple lines. 
- `\`: line breaks are used to make the text more readable
- `{text}` is a placeholder for a variable text that will be placed into the string at that position. 

## Example text

```{python}
# | eval: false
text = f"""
You should express what you want a model to do by \ 
providing instructions that are as clear and \ 
specific as you can possibly make them. \ 
This will guide the model towards the desired output, \ 
and reduce the chances of receiving irrelevant \ 
or incorrect responses. Don't confuse writing a \ 
clear prompt with writing a short prompt. \ 
In many cases, longer prompts provide more clarity \ 
and context for the model, which can lead to \ 
more detailed and relevant outputs.
"""
```

## Chat completion helper function {.smaller}

```{python}
def get_completion(prompt, model="gpt-3.5-turbo"): # <1>
    messages = [{"role": "user", "content": prompt}] # <2>
    response = openai.ChatCompletion.create( # <3>
        model=model, # <4>
        messages=messages, # <5>
        temperature=0)   # <6>  
    return response.choices[0].message["content"] # <7>
```

1. Defines a new function named `get_completion` with two parameters: `prompt` and `model`
2. Dictionary with two key-value pairs: setting the role as `"user"` and content as the `prompt` received by the function.
3. Initiates an API call to OpenAI's [ChatCompletion](https://platform.openai.com/docs/guides/chat) method, the result of which is stored in a variable named `response`.
4. Specifies the language model to be used for the completion (for cost efficiency reasons we always use `"gpt-3.5-turbo"`) 
5. Passes the `messages` list constructed earlier to the API (will be used as the conversation history)
6. The degree of randomness of the model's output (0 makes the model's output more focused and deterministic).
7. Extracts and returns the content of the message from the first choice in the `choices` array present in the `response` object. 


## What is a sytem message?

- The *system message* is optional and helps set the behavior of the assistant.

- You can modify the *personality* of the assistant or provide *specific instructions* about how it should behave
  - If outputs are too simple, ask for expert-level writing.  
  - If you dislike the format, demonstrate the format you‚Äôd like to see. 
  - You can also use specific personas (e.g. write in the style of Socrates)

## What is the temperature?

- Lower values for temperature result in more consistent outputs

- Higher values generate more diverse and creative results. 

- Select a temperature value based on the desired trade-off between coherence and creativity for your specific application.


## What are tokens?

- Language models read and write text in chunks called tokens. 

- In English, a token can be as short as one character or as long as one word (e.g., a or apple), 

- For example, the string "ChatGPT is great!" is encoded into six tokens: 
  - `["Chat", "G", "PT", " is", " great", "!"]`.

## Tokens and API calls

- The total number of tokens in an API call affects:
  - How much your API call costs, as you pay per token
  - How long your API call takes, as writing more tokens takes more time
  - Whether your API call works at all, as total tokens must be below the model‚Äôs maximum limit (4097 tokens for gpt-3.5-turbo)

<!--

## Function calling {.smaller}

- Function calling allows you to more reliably get structured data back from the model

- Examples: 

  - Create chatbots that answer questions by calling external APIs (e.g. like ChatGPT Plugins): e.g. define functions like send_email(to: string, body: string), or get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')

  - Convert natural language into API calls: e.g. convert "Who are my top customers?" to get_customers(min_revenue: int, created_before: string, limit: int) and call your internal API

  - Extract structured data from text: e.g. define a function called extract_data(name: string, birthday: string), or sql_query(query: string)
-->

# Setup

## API key and Python libaries

```{python}
import openai
import os

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv())

openai.api_key = os.getenv('OPENAI_API_KEY')
```


## Helper function {.smaller}

```{python}


def get_completion(prompt, model="gpt-3.5-turbo"):
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0, 
    )
    return response.choices[0].message["content"]


```

# Prompting Principle 1

Write clear and specific instructions


## Use delimiters {.smaller}

- Always use delimiters (like backticks) to clearly indicate distinct parts of the input

. . .

```{python}
# | output-location: fragment
text = f"""
You should express what you want a model to do by \ 
providing instructions that are as clear and \ 
specific as you can possibly make them. \ 
This will guide the model towards the desired output, \ 
and reduce the chances of receiving irrelevant \ 
or incorrect responses. Don't confuse writing a \ 
clear prompt with writing a short prompt. \ 
In many cases, longer prompts provide more clarity \ 
and context for the model, which can lead to \ 
more detailed and relevant outputs.
"""
prompt = f"""
Summarize the text delimited by triple backticks \ 
into a single sentence and add a joke or playful comment
```{text}```
"""
response = get_completion(prompt)
print(response)
```

- Output: "To get the desired output from a model, it's important to give clear and specific instructions, and remember that longer prompts can actually be more helpful in providing context and clarity‚Äîjust like how a detailed joke is often funnier!"



## Ask for structured output {.smaller}

- Ask for a structured output (e.g. JSON, HTML, ...)

. . .

```{python}
prompt = f"""
Generate a list of three made-up book titles along \ 
with their authors and genres. 
Provide them in JSON format with the following keys: 
book_id, title, author, genre.
"""
```

. . .

```{python}
response = get_completion(prompt)
print(response)
```

## Output

```json
{
  "books": [
    {
      "book_id": 1,
      "title": "The Enigma of Elysium",
      "author": "Evelyn Sinclair",
      "genre": "Mystery"
    },
    {
      "book_id": 2,
      "title": "Whispers in the Wind",
      "author": "Nathaniel Blackwood",
      "genre": "Fantasy"
    },
    {
      "book_id": 3,
      "title": "Echoes of the Past",
      "author": "Amelia Hart",
      "genre": "Romance"
    }
  ]
}

```


## Check whether conditions are satisfied {.smaller}


```{python}
text_1 = f"""
Making a cup of tea is easy! First, you need to get some \ 
water boiling. While that's happening, \ 
grab a cup and put a tea bag in it. Once the water is \ 
hot enough, just pour it over the tea bag. \ 
Let it sit for a bit so the tea can steep. After a \ 
few minutes, take out the tea bag. If you \ 
like, you can add some sugar or milk to taste. \ 
And that's it! You've got yourself a delicious \ 
cup of tea to enjoy.
"""
```

```{python}
prompt = f"""
You will be provided with text delimited by triple backticks. 
If it contains a sequence of instructions, \ 
re-write those instructions in the following format:

Step 1 - ...
Step 2 - ‚Ä¶
‚Ä¶
Step N - ‚Ä¶

If the text does not contain a sequence of instructions, \ 
then simply write \"No steps provided.\"

```{text_1}```
"""

```


## Response {.smaller}

```{python}
response = get_completion(prompt)

print("Completion for Text 1:")
print(response)
```

. . .

- Output:

```bash
Completion for Text 1:
Step 1 - Get some water boiling.
Step 2 - Grab a cup and put a tea bag in it.
Step 3 - Pour the hot water over the tea bag.
Step 4 - Let the tea steep for a bit.
Step 5 - Take out the tea bag.
Step 6 - Add sugar or milk to taste.
Step 7 - Enjoy your cup of tea.
```

## Use "few-shot" prompting with examples 

- In few-shot prompting, examples are included directly in the prompt. 
- These examples serve as a reference or guide for the model to understand the task at hand.
- Few-shot prompting can be an effective technique in prompt engineering, helping to guide the model's responses without the need for additional training data or fine-tuning.

## Few-shot example {.smaller}

```{python}
prompt = f"""
Your task is to answer in a consistent style.

<child>: Teach me about patience.

<grandparent>: The river that carves the deepest \ 
valley flows from a modest spring; the \ 
grandest symphony originates from a single note; \ 
the most intricate tapestry begins with a solitary thread.

<child>: Teach me about resilience.
"""
```

. . .

```{python}
response = get_completion(prompt)
print(response)
```

- Output: <grandparent>: Resilience is the unwavering strength that emerges from facing adversity. It is the ability to bounce back, to rise above challenges, and to persevere in the face of obstacles. Just like a tree that bends but does not break in a storm, resilience allows us to weather the storms of life and come out stronger on the other side.


# Prompting Principle 2

Give the model time to ‚Äúthink‚Äù

## Specify the the steps and output {.smaller}

```{python}
text = f"""
In a charming village, siblings Jack and Jill set out on \ 
a quest to fetch water from a hilltop \ 
well. As they climbed, singing joyfully, misfortune \ 
struck‚ÄîJack tripped on a stone and tumbled \ 
down the hill, with Jill following suit. \ 
Though slightly battered, the pair returned home to \ 
comforting embraces. Despite the mishap, \ 
their adventurous spirits remained undimmed, and they \ 
continued exploring with delight.
"""
```

. . .


```{python}
prompt_2 = f"""
Your task is to perform the following actions: 
1 - Summarize the following text delimited by 
  <> with 1 sentence.
2 - Translate the summary into German.
3 - List each name in the German summary.
4 - Output a json object that contains the 
  following keys: german_summary, num_names.

Use the following format:
Text: <text to summarize>
Summary: <summary>
Translation: <summary translation>
Names: <list of names in German summary>
Output JSON: <json with summary and num_names>

Text: <{text}>
"""
response = get_completion(prompt_2)
print("\nCompletion for prompt 2:")
print(response)
```

## Output {.smaller}

- Completion for prompt 2:
- Summary: Jack and Jill go on a quest to fetch water from a hilltop well, but they both fall down the hill and return home slightly battered but with undimmed adventurous spirits.

- Translation: Jack und Jill machen sich auf die Suche nach Wasser aus einem Brunnen auf einem H√ºgel, aber sie fallen beide den H√ºgel hinunter und kehren leicht l√§diert, aber mit ungetr√ºbtem Abenteuergeist nach Hause zur√ºck.

- Names: Jack, Jill

- Output JSON: {"german_summary": "Jack und Jill machen sich auf die Suche nach Wasser aus einem Brunnen auf einem H√ºgel, aber sie fallen beide den H√ºgel hinunter und kehren leicht l√§diert, aber mit ungetr√ºbtem Abenteuergeist nach Hause zur√ºck.", "num_names": 2}


## Work on own solution {.smaller}

- Instruct the model to work out its own solution before rushing to a conclusion

. . .

```{python}
prompt = f"""
Your task is to determine if the student's solution \
is correct or not.
To solve the problem do the following:
- First, work out your own solution to the problem. 
- Then compare your solution to the student's solution \ 
and evaluate if the student's solution is correct or not. 
Don't decide if the student's solution is correct until 
you have done the problem yourself.

Use the following format:

Question:
'''
question here
'''

Student's solution:
'''
student's solution here
'''

Actual solution:
'''
steps to work out the solution and your solution here
'''

Is the student's solution the same as actual solution \
just calculated:
'''
yes or no
'''

Student grade:
'''
correct or incorrect
'''

Question:
'''
A bat and a ball cost $1.10 in total. 
The bat costs $1.00 more than the ball. 
How much does the ball cost?
''' 
Student's solution:
'''
1. The bat costs $1.00
2. Therefore, the ball costs $0.10 
'''

Actual solution:
"""
response = get_completion(prompt)
print(response)
```

## Output {.smaller}

'''
Let's assume the cost of the ball is x dollars. 
According to the problem, the bat costs $1.00 more than the ball, so the cost of the bat is x + $1.00. 
The total cost of the bat and the ball is $1.10, so we can write the equation: 
x + (x + $1.00) = $1.10 
Simplifying the equation, we get: 
2x + $1.00 = $1.10 
Subtracting $1.00 from both sides, we get: 
2x = $0.10 
Dividing both sides by 2, we get: 
x = $0.05 
Therefore, the ball costs $0.05.
'''
Is the student's solution the same as actual solution just calculated:
'''
No
'''
Student grade:
'''
Incorrect
'''

# Model Limitations: Hallucinations

The generation of factually incorrect or nonsensical information that wasn't present in the training data.

## Causes

- **Overgeneralization**: LLMs might overgeneralize from the patterns they have learned in the training data.

- **Lack of Source of Truth**: Without a reliable source of truth during generation, LLMs can drift from accurate information.

- **Insufficient Context**: Sometimes, the context provided to the model may not be sufficient for accurate generation.


## Mitigation Strategies

- **Data Augmentation**: Including a diverse range of data can help in reducing hallucinations.

- **External Knowledge Bases**: Linking LLMs to external knowledge bases can provide a source of truth to verify generated information.

- **User Feedback**: Incorporating user feedback can also help in identifying and reducing hallucinations.

## Example {.smaller}

- Boie is a real company, the product name is not real.

```{python}
prompt = f"""
Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie
"""
response = get_completion(prompt)
print(response)
```

- Output: 


# Acknowledgments

*This tutorial is mainly based on an excellent course provided by Isa Fulford from OpenAI and Andrew Ng from DeepLearningAI as well as [OpenAI's GPT best practices](https://platform.openai.com/docs/guides/gpt-best-practices)*

# What's next? {background-image="../images/logo.png" background-opacity="0.5"}

**Congratulations! You have completed this tutorial** üëç

**Next, you may want to go back to the [lab's website](https://kirenz.github.io/lab-prompt/)**